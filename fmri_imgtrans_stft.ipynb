{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/short_range/padded_fmri_set.pt\n",
      "dataset ready\n"
     ]
    }
   ],
   "source": [
    "from models.model import ImagenTime\n",
    "from utils.utils_data import gen_dataloader\n",
    "from utils.utils import restore_state\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from models.sampler import DiffusionProcess\n",
    "%matplotlib inline\n",
    "import torch\n",
    "\n",
    "# Define args as needed for gen_dataloader\n",
    "class Args:\n",
    "\tdef __init__(self):\n",
    "\t\tself.batch_size = 69696 # check this \n",
    "\t\tself.shuffle = True\n",
    "\t\tself.num_workers = 4\n",
    "\t\tself.dataset = \"fmri\"    \n",
    "\t\tself.device = \"cuda\"\n",
    "\t\tself.use_stft = True\n",
    "\t\tself.diffusion_steps = 18\n",
    "\t\tself.n_fft = 63\n",
    "\t\tself.hop_length = 23\n",
    "\t\tself.img_resolution = 32\n",
    "\t\t\n",
    "\t\tself.input_channels = 2\n",
    "\t\tself.unet_channels = 128\n",
    "\t\tself.ch_mult = [1,2,4,4]\n",
    "\t\tself.attn_resolution = [32,16,8]\n",
    "\t\tself.ema = True\n",
    "\t\tself.ema_warmup = 100\n",
    "\t\tself.logging_iter = 100\n",
    "\t\tself.learning_rate: 0.0003 #1e-4\n",
    "\t\tself.weight_decay: 0.00001 #1e-5\n",
    "# \n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "train_loader, test_loader = gen_dataloader(args)\n",
    "print(\"dataset ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "from utils.utils_data import MinMaxScaler, MinMaxArgs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from contextlib import contextmanager\n",
    "from models.networks import EDMPrecond\n",
    "from models.ema import LitEma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TsImgEmbedder(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for transforming time series to images and vice versa\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, seq_len):\n",
    "        self.device = device\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    @abstractmethod\n",
    "    def ts_to_img(self, signal):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            signal: given time series\n",
    "\n",
    "        Returns:\n",
    "            image representation of the signal\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def img_to_ts(self, img):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            img: given generated image\n",
    "\n",
    "        Returns:\n",
    "            time series representation of the generated image\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class STFTEmbedder(TsImgEmbedder):\n",
    "    \"\"\"\n",
    "    STFT transformation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, seq_len, n_fft, hop_length):\n",
    "        super().__init__(device, seq_len)\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.min_real, self.max_real, self.min_imag, self.max_imag = None, None, None, None\n",
    "\n",
    "    def cache_min_max_params(self, train_data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_data: training timeseries dataset. shape: B*L*K\n",
    "        this function initializes the min and max values for the real and imaginary parts.\n",
    "        we'll use this function only once, before the training loop starts.\n",
    "        \"\"\"\n",
    "        real, imag = self.stft_transform(train_data)\n",
    "        # compute and cache min and max values\n",
    "        real, min_real, max_real = MinMaxScaler(real.numpy(), True)\n",
    "        imag, min_imag, max_imag = MinMaxScaler(imag.numpy(), True)\n",
    "        self.min_real, self.max_real = torch.Tensor(min_real), torch.Tensor(max_real)\n",
    "        self.min_imag, self.max_imag = torch.Tensor(min_imag), torch.Tensor(max_imag)\n",
    "\n",
    "    def stft_transform(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: time series data. Shape: B*L*K\n",
    "        Returns:\n",
    "            real and imaginary parts of the STFT transformation\n",
    "        \"\"\"\n",
    "        data = torch.permute(data, (0, 2, 1))  # we permute to match requirements of torchaudio.transforms.Spectrogram\n",
    "        spec = T.Spectrogram(n_fft=self.n_fft, hop_length=self.hop_length, center=True, power=None).to(data.device)\n",
    "        transformed_data = spec(data)\n",
    "        return transformed_data.real, transformed_data.imag\n",
    "\n",
    "    def ts_to_img(self, signal):\n",
    "        assert self.min_real is not None, \"use init_norm_args() to compute scaling arguments\"\n",
    "        # convert to complex spectrogram\n",
    "        real, imag = self.stft_transform(signal)\n",
    "        # MinMax scaling\n",
    "        real = (MinMaxArgs(real, self.min_real.to(self.device), self.max_real.to(self.device)) - 0.5) * 2\n",
    "        imag = (MinMaxArgs(imag, self.min_imag.to(self.device), self.max_imag.to(self.device)) - 0.5) * 2\n",
    "        # stack real and imag parts\n",
    "        stft_out = torch.cat((real, imag), dim=1)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return stft_out\n",
    "\n",
    "    def img_to_ts(self, x_image):\n",
    "        n_fft = self.n_fft\n",
    "        hop_length, length = self.hop_length, self.seq_len\n",
    "        min_real, max_real, min_imag, max_imag = self.min_real.to(\n",
    "            self.device), self.max_real.to(\n",
    "            self.device), \\\n",
    "            self.min_imag.to(self.device), self.max_imag.to(\n",
    "            self.device)\n",
    "        # -- combine real and imaginary parts --\n",
    "        split = torch.split(x_image, x_image.shape[1] // 2,\n",
    "                            dim=1)  # x_image.shape[1] is twice the size of the original dim\n",
    "\n",
    "        real, imag = split[0], split[1]\n",
    "        \n",
    "        print(\"x_image shape:\", x_image.shape)\n",
    "        print(\"real shape:\", real.shape)\n",
    "        print(\"imag shape:\", imag.shape)\n",
    "        print(\"max_real shape:\", max_real.shape)\n",
    "        print(\"min_real shape:\", min_real.shape)\n",
    "\n",
    "        unnormalized_real = ((real / 2) + 0.5) * (max_real - min_real) + min_real\n",
    "        unnormalized_imag = ((imag / 2) + 0.5) * (max_imag - min_imag) + min_imag\n",
    "        unnormalized_stft = torch.complex(unnormalized_real, unnormalized_imag)\n",
    "        # -- inverse stft --\n",
    "        ispec = T.InverseSpectrogram(n_fft=n_fft, hop_length=hop_length, center=True).to(self.device)\n",
    "\n",
    "        x_time_series = ispec(unnormalized_stft, length)\n",
    "\n",
    "        return torch.permute(x_time_series, (0, 2, 1))  # B*L*K(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenTime(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        '''\n",
    "        beta_1    : beta_1 of diffusion process\n",
    "        beta_T    : beta_T of diffusion process\n",
    "        T         : Diffusion Steps\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        self.P_mean = -1.2\n",
    "        self.P_std = 1.2\n",
    "        self.sigma_data = 0.5\n",
    "        self.sigma_min = 0.002\n",
    "        self.sigma_max = 80\n",
    "        self.rho = 7\n",
    "        self.T = args.diffusion_steps\n",
    "\n",
    "        self.device = device\n",
    "        self.net = EDMPrecond(args.img_resolution, args.input_channels, channel_mult=args.ch_mult,\n",
    "                              model_channels=args.unet_channels, attn_resolutions=args.attn_resolution)\n",
    "\n",
    "        # delay embedding is used\n",
    "        if not args.use_stft:\n",
    "            self.delay = args.delay\n",
    "            self.embedding = args.embedding\n",
    "            self.seq_len = args.seq_len\n",
    "\n",
    "            # NOTE: added this\n",
    "            # self.ts_img = DelayEmbedder(self.device, args.seq_len, args.delay, args.embedding)\n",
    "        else:\n",
    "            self.ts_img = STFTEmbedder(self.device, args.seq_len, args.n_fft, args.hop_length)\n",
    "\n",
    "        if args.ema:\n",
    "            self.use_ema = True\n",
    "            self.model_ema = LitEma(self.net, decay=0.9999, use_num_upates=True, warmup=args.ema_warmup)\n",
    "        else:\n",
    "            self.use_ema = False\n",
    "\n",
    "    def ts_to_img(self, signal, pad_val=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            signal: signal to convert to image\n",
    "            pad_val: value to pad the image with, if delay embedding is used. Do not use for STFT embedding\n",
    "\n",
    "        \"\"\"\n",
    "        # pad_val is used only for delay embedding, as the value to pad the image with\n",
    "        # when creating the mask, we need to use 1 as padding value\n",
    "        # if pad_val is given, it is used to overwrite the default value of 0\n",
    "\n",
    "        # print(pad_val)\n",
    "\n",
    "\n",
    "        return self.ts_img.ts_to_img(signal, True, pad_val) if pad_val else self.ts_img.ts_to_img(signal)\n",
    "\n",
    "    def img_to_ts(self, img):\n",
    "        return self.ts_img.img_to_ts(img)\n",
    "\n",
    "    # init the min and max values for the STFTEmbedder, this function must be called before the training loop starts\n",
    "    def init_stft_embedder(self, train_loader):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_loader: training data\n",
    "\n",
    "        caches min and max values for the real and imaginary parts\n",
    "        of the STFT transformation, which will be used for normalization.\n",
    "        \"\"\"\n",
    "        assert type(self.ts_img) == STFTEmbedder, \"You must use the STFTEmbedder to initialize the min and max values\"\n",
    "        data = []\n",
    "        for i, data_batch in enumerate(train_loader):\n",
    "            data.append(data_batch[0])\n",
    "        self.ts_img.cache_min_max_params(torch.cat(data, dim=0))\n",
    "\n",
    "    def loss_fn(self, x):\n",
    "        '''\n",
    "        x          : real data if idx==None else perturbation data\n",
    "        idx        : if None (training phase), we perturbed random index.\n",
    "        '''\n",
    "\n",
    "        to_log = {}\n",
    "\n",
    "        output, weight = self.forward(x)\n",
    "\n",
    "        # denoising matching term\n",
    "        # loss = weight * ((output - x) ** 2)\n",
    "        loss = (weight * (output - x).square()).mean()\n",
    "        to_log['karras loss'] = loss.detach().item()\n",
    "\n",
    "        return loss, to_log\n",
    "\n",
    "    def loss_fn_impute(self, x, mask):\n",
    "        '''\n",
    "        x          : real data if idx==None else perturbation data\n",
    "        idx        : if None (training phase), we perturbed random index.\n",
    "        '''\n",
    "\n",
    "        to_log = {}\n",
    "        output, weight = self.forward_impute(x, mask)\n",
    "        x = self.unpad(x * (1 - mask), x.shape)\n",
    "        output = self.unpad(output * (1 - mask), x.shape)\n",
    "        loss = (weight * (output - x).square()).mean()\n",
    "        to_log['karras loss'] = loss.detach().item()\n",
    "\n",
    "        return loss, to_log\n",
    "\n",
    "\n",
    "    def forward(self, x, labels=None, augment_pipe=None):\n",
    "\n",
    "        rnd_normal = torch.randn([x.shape[0], 1, 1, 1], device=x.device)\n",
    "        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n",
    "        weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data) ** 2\n",
    "        y, augment_labels = augment_pipe(x) if augment_pipe is not None else (x, None)\n",
    "        n = torch.randn_like(y) * sigma\n",
    "        D_yn = self.net(y + n, sigma, labels, augment_labels=augment_labels)\n",
    "        return D_yn, weight\n",
    "\n",
    "    def forward_impute(self, x, mask, labels=None, augment_pipe=None):\n",
    "\n",
    "        rnd_normal = torch.randn([x.shape[0], 1, 1, 1], device=x.device)\n",
    "        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n",
    "        weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data) ** 2\n",
    "\n",
    "        # noisy impute part\n",
    "        n = torch.randn_like(x) * sigma\n",
    "        noise_impute = n * (1 - mask)\n",
    "        x_to_impute = x * (1 - mask) + noise_impute\n",
    "\n",
    "        # clear image\n",
    "        x = x * mask\n",
    "        y, augment_labels = augment_pipe(x) if augment_pipe is not None else (x, None)\n",
    "\n",
    "        D_yn = self.net(y + x_to_impute, sigma, labels, augment_labels=augment_labels)\n",
    "        return D_yn, weight\n",
    "\n",
    "    def forward_forecast(self, past, future, labels=None, augment_pipe=None):\n",
    "        s, e = past.shape[-1], future.shape[-1]\n",
    "        rnd_normal = torch.randn([past.shape[0], 1, 1, 1], device=past.device)\n",
    "        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n",
    "        weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data) ** 2\n",
    "        y, augment_labels = augment_pipe(past) if augment_pipe is not None else (past, None)\n",
    "        n = torch.randn_like(future) * sigma\n",
    "        full_seq = self.pad_f(torch.cat([past, future + n], dim=-1))\n",
    "        D_yn = self.net(full_seq, sigma, labels, augment_labels=augment_labels)[..., s:(s + e)]\n",
    "        return D_yn, weight\n",
    "\n",
    "    def pad_f(self, x):\n",
    "        \"\"\"\n",
    "        Pads the input tensor x to make it square along the last two dimensions.\n",
    "        \"\"\"\n",
    "        _, _, cols, rows = x.shape\n",
    "        max_side = max(32, rows)\n",
    "        padding = (\n",
    "            0, max_side - rows, 0, 0)  # Padding format: (pad_left, pad_right, pad_top, pad_bottom)\n",
    "\n",
    "        # Padding the last two dimensions to make them square\n",
    "        x_padded = torch.nn.functional.pad(x, padding, mode='constant', value=0)\n",
    "        return x_padded\n",
    "\n",
    "    def unpad(self, x, original_shape):\n",
    "        \"\"\"\n",
    "        Removes the padding from the tensor x to get back to its original shape.\n",
    "        \"\"\"\n",
    "        _, _, original_cols, original_rows = original_shape\n",
    "        return x[:, :, :original_cols, :original_rows]\n",
    "\n",
    "    @contextmanager\n",
    "    def ema_scope(self, context=None):\n",
    "        \"\"\"\n",
    "        Context manager to temporarily switch to EMA weights during inference.\n",
    "        Args:\n",
    "            context: some string to print when switching to EMA weights\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        if self.use_ema:\n",
    "            self.model_ema.store(self.net.parameters())\n",
    "            self.model_ema.copy_to(self.net)\n",
    "            if context is not None:\n",
    "                print(f\"{context}: Switched to EMA weights\")\n",
    "        try:\n",
    "            yield None\n",
    "        finally:\n",
    "            if self.use_ema:\n",
    "                self.model_ema.restore(self.net.parameters())\n",
    "                if context is not None:\n",
    "                    print(f\"{context}: Restored training weights\")\n",
    "\n",
    "    def on_train_batch_end(self, *args):\n",
    "        \"\"\"\n",
    "        this function updates the EMA model, if it is used\n",
    "        Args:\n",
    "            *args:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        if self.use_ema:\n",
    "            self.model_ema(self.net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT embedder initialized\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.device = device\n",
    "\n",
    "model = ImagenTime(args=args, device=args.device).to(args.device)\n",
    "if args.use_stft:\n",
    "    model.init_stft_embedder(train_loader)\n",
    "    print(\"STFT embedder initialized\")\n",
    "\n",
    "args.learning_rate=  0.0003 #1e-4\n",
    "args.weight_decay=  0.00001 #1e-4\n",
    "args.resume = False\n",
    "args.epochs = 1000\n",
    "args.beta1 = 1e-05\n",
    "args.betaT = 0.01\n",
    "args.deterministic = False\n",
    "\n",
    "\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "state = dict(model=model, epoch=0)\n",
    "init_epoch = 0\n",
    "\n",
    "# restore checkpoint\n",
    "if args.resume:\n",
    "    ema_model = model.model_ema if args.ema else None # load ema model if available\n",
    "    init_epoch = restore_state(args, state, ema_model=ema_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(init_epoch, args.epochs):\n",
    "    model.train()\n",
    "    model.epoch = epoch\n",
    "\n",
    "    # --- train loop ---\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        x_ts = data[0].to(args.device)  # x_ts contains the time series batch\n",
    "\n",
    "  \n",
    "\n",
    "        x_img = model.ts_to_img(x_ts)\n",
    "\n",
    "\n",
    "\n",
    "        # # -HeatMap Features Vs Time Steps - p0\n",
    "        # data_squeezed = x_ts.squeeze(-1)  # Remove the singleton dimension\n",
    "        # plt.figure(figsize=(12, 6))\n",
    "        # sns.heatmap(data_squeezed.numpy(), cmap=\"viridis\", cbar=True)\n",
    "\n",
    "        # # Add labels and title\n",
    "        # plt.xlabel(\"Feature Index\")\n",
    "        # plt.ylabel(\"Time Steps\")\n",
    "        # plt.title(\"Features vs. Time Steps (Heatmap)\")\n",
    "        # plt.savefig('Ts_heatmap_fmri_pre.png')\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        # - Take mean of last dimension method\n",
    "        # Step 1: Reshape the data to (17, 264, 264)\n",
    "        # data_reshaped = x_ts.view(17, 264, 264)  # Reshaping (69696 -> 264 x 264)\n",
    "\n",
    "        # # Step 2: Compute the mean along the last dimension (mean of each 264 group)\n",
    "        # mean_features = data_reshaped.mean(dim=2)  # Shape: (17, 264)\n",
    "\n",
    "        # # Step 3: Plot the 264 lines\n",
    "        # plt.figure(figsize=(12, 8))\n",
    "        # for feature_idx in range(264):\n",
    "        #     plt.plot(range(17), mean_features[:, feature_idx].numpy(), label=f\"Group {feature_idx + 1}\")\n",
    "\n",
    "        # # Add labels, title, and grid\n",
    "        # plt.xlabel(\"Time Steps\")\n",
    "        # plt.ylabel(\"Mean Feature Value\")\n",
    "        # plt.title(\"Mean of 264 Groups of Features Across Time Steps\")\n",
    "        # plt.grid(True)\n",
    "\n",
    "        # Optional: Add a legend (only if you want to show it for some groups)\n",
    "        # plt.legend(loc=\"upper right\", fontsize=\"small\", ncol=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # -Density plot\n",
    "\n",
    "        # # Select a specific time step\n",
    "        # time_step_idx = 0\n",
    "        # features = x_ts[time_step_idx, :, 0].numpy()  # Extract all features at the selected time step\n",
    "        # time_steps_to_compare = [0, 8, 16]\n",
    "\n",
    "        # plt.figure(figsize=(12, 6))\n",
    "        # for t in time_steps_to_compare:\n",
    "        #     features = x_ts[t, :, 0].numpy()\n",
    "        #     sns.kdeplot(features, label=f\"Time Step {t}\", alpha=0.6)\n",
    "\n",
    "        # plt.xlabel(\"Feature Values\")\n",
    "        # plt.ylabel(\"Density\")\n",
    "        # plt.title(\"Feature Value Distributions Across Time Steps\")\n",
    "        # plt.legend()\n",
    "        # plt.grid(True)\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        # ts -> img done\n",
    "        \n",
    "        # img_sample = x_img[0, 0, :, :]  # Shape: (height, width)\n",
    "        # # Convert to NumPy for visualization\n",
    "        # img_numpy = img_sample.cpu().detach().numpy()\n",
    "        # # Plot the spectrogram\n",
    "        # plt.figure(figsize=(10, 5))\n",
    "        # plt.imshow(img_numpy, aspect='auto', origin='lower', cmap='viridis')\n",
    "        # plt.colorbar(label='Intensity')\n",
    "        # plt.title(\"Spectrogram (Time vs Frequency)\")\n",
    "        # plt.xlabel(\"Time Steps\")\n",
    "        # plt.ylabel(\"Frequency Bins\")\n",
    "        # plt.savefig('spectrogram_fmri.png')\n",
    "        # plt.show()\n",
    "\n",
    "        \n",
    "    # break\n",
    "\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        # loss = model.loss_fn(x_img)\n",
    "        # if len(loss) == 2:\n",
    "        #     loss, to_log = loss\n",
    "        #     for key, value in to_log.items():\n",
    "        #         print(f'train/{key}', value, epoch)\n",
    "\n",
    "        # loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        # optimizer.step()\n",
    "        # model.on_train_batch_end()\n",
    "        break\n",
    "\n",
    "    # # --- evaluation loop ---\n",
    "    # if epoch % args.logging_iter == 0:\n",
    "    #     gen_sig = []\n",
    "    #     real_sig = []\n",
    "    #     model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         with model.ema_scope():\n",
    "    #             process = DiffusionProcess(args, model.net,\n",
    "    #                                         (args.input_channels, args.img_resolution, args.img_resolution))\n",
    "    #             for data in tqdm(test_loader):\n",
    "    #                 # sample from the model\n",
    "    #                 x_img_sampled = process.sampling(sampling_number=data[0].shape[0])\n",
    "    #                 # --- convert to time series --\n",
    "    #                 x_ts = model.img_to_ts(x_img_sampled)\n",
    "                    \n",
    "\n",
    "    #                 break\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
